{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE CODE TAKEN FORM YT\n",
    "https://www.youtube.com/watch?v=cA8K8dl-E6k&t=2477s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caution\n",
    "\n",
    "I did test many diff images things learned:\n",
    "\n",
    "1. Have images with 80-90% same features look at the dataset\n",
    "2. perspective could be diff\n",
    "3. use the pts parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated homography : \n",
      " [[ 6.98423564e-01 -4.85089541e-02  8.14785696e+01]\n",
      " [-2.19601494e-02  1.36061383e+00 -3.92380746e+01]\n",
      " [-1.69529488e-06 -2.31555154e-04  1.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "im1 = cv2.imread('./image_registration_dataset/rio_register.jpg')          # Image that needs to be registered.\n",
    "im2 = cv2.imread('./image_registration_dataset/rio_base_1.jpg') # trainImage\n",
    "\n",
    "im1 = cv2.resize(im1, (600, 600), interpolation= cv2.INTER_LINEAR)\n",
    "im2 = cv2.resize(im2, (600, 600), interpolation= cv2.INTER_LINEAR)\n",
    "\n",
    "\n",
    "img1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "img2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initiate ORB detector\n",
    "pts = 50 # your hyper param mostly increase it\n",
    "orb = cv2.ORB_create(pts)  #Registration works with at least 50 points\n",
    "\n",
    "# find the keypoints and descriptors with orb\n",
    "kp1, des1 = orb.detectAndCompute(img1, None)  #kp1 --> list of keypoints\n",
    "kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "#Brute-Force matcher takes the descriptor of one feature in first set and is \n",
    "#matched with all other features in second set using some distance calculation.\n",
    "# create Matcher object\n",
    "\n",
    "matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "\n",
    "# Match descriptors.\n",
    "matches = matcher.match(des1, des2, None)  #Creates a list of all matches, just like keypoints\n",
    "\n",
    "# Sort them in the order of their distance.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "#Like we used cv2.drawKeypoints() to draw keypoints, \n",
    "#cv2.drawMatches() helps us to draw the matches. \n",
    "#https://docs.opencv.org/3.0-beta/modules/features2d/doc/drawing_function_of_keypoints_and_matches.html\n",
    "# Draw first 10 matches.\n",
    "img3 = cv2.drawMatches(im1,kp1, im2, kp2, matches[:10], None)\n",
    "\n",
    "cv2.imshow(\"Matches image\", img3)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#Now let us use these key points to register two images. \n",
    "#Can be used for distortion correction or alignment\n",
    "#For this task we will use homography. \n",
    "# https://docs.opencv.org/3.4.1/d9/dab/tutorial_homography.html\n",
    "\n",
    "# Extract location of good matches.\n",
    "# For this we will use RANSAC.\n",
    "#RANSAC is abbreviation of RANdom SAmple Consensus, \n",
    "#in summary it can be considered as outlier rejection method for keypoints.\n",
    "#http://eric-yuan.me/ransac/\n",
    "#RANSAC needs all key points indexed, first set indexed to queryIdx\n",
    "#Second set to #trainIdx. \n",
    "\n",
    "points1 = np.zeros((len(matches), 2), dtype=np.float32)  #Prints empty array of size equal to (matches, 2)\n",
    "points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "for i, match in enumerate(matches):\n",
    "   points1[i, :] = kp1[match.queryIdx].pt    #gives index of the descriptor in the list of query descriptors\n",
    "   points2[i, :] = kp2[match.trainIdx].pt    #gives index of the descriptor in the list of train descriptors\n",
    "\n",
    "#Now we have all good keypoints so we are ready for homography.   \n",
    "# Find homography\n",
    "#https://en.wikipedia.org/wiki/Homography_(computer_vision)\n",
    "  \n",
    "h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    " \n",
    "  # Use homography\n",
    "height, width, channels = im2.shape\n",
    "im1Reg = cv2.warpPerspective(im1, h, (width, height))  #Applies a perspective transformation to an image.\n",
    "   \n",
    "print(\"Estimated homography : \\n\",  h)\n",
    "\n",
    "cv2.imshow(\"Registered image\", im1Reg)\n",
    "cv2.waitKey()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
